{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_rnn_book_writer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devWithDeepak/dl_notebooks/blob/master/char_rnn_book_writer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "twwuDc9RNfR6",
        "colab_type": "code",
        "outputId": "a7e8d677-2ea4-4881-8983-2f3b80b8b42b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i0slO-hWNdGV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ve-sOHBnNdGz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    x = np.eye(n_labels)[arr]\n",
        "    return x.astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktqkrvT5QZd8",
        "colab_type": "code",
        "outputId": "2a291875-d223-4955-9503-b7a316ec5588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "arr=[3,4,5]\n",
        "print(one_hot_encode(arr, 8))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZGEUZwjkNdHG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    total_batch_size = batch_size * seq_length\n",
        "    \n",
        "    # get total number of batches\n",
        "    n_batches = len(arr) // total_batch_size\n",
        "    \n",
        "    # charactes included\n",
        "    arr = arr[:n_batches * total_batch_size]\n",
        "    \n",
        "    # resize arr to batch size\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # features\n",
        "        x = arr[: , n: n + seq_length]\n",
        "        # targets\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[: , 1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[: , 1] = x[:, 1:], arr[:, 0]\n",
        "            \n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J_VrftU6NdHX",
        "colab_type": "code",
        "outputId": "ad79b12c-d35b-410d-a25a-23d2d4105c35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# check if cuda is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print(\"Training on GPU\")\n",
        "else:\n",
        "    print(\"Training on CPU\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U9RLIfa4NdHr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.001):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create dictionary from text\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch:ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        # create instance variable\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_prob = drop_prob\n",
        "        self.lr = lr\n",
        "        \n",
        "        # define lstm layer\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # define fully connected layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        # lstm layer \n",
        "        x=x.float()\n",
        "        r_out, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        # dropout layer\n",
        "        r_out = self.dropout(r_out)\n",
        "\n",
        "        # reshape output\n",
        "        r_out = r_out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        # fully connected layer\n",
        "        output = self.fc(r_out)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        # create two tensors for two hidden layer in lstm\n",
        "        if(train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m37hrGgbNdH3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train function\n",
        "def train(net, data, n_epochs = 10, batch_size=10, seq_length=50,\n",
        "          lr=0.01, val_split = 0.1, clip=5, print_every=10):\n",
        "    # enable training mode\n",
        "    net.train()\n",
        "    \n",
        "    # define loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    \n",
        "    # validation index\n",
        "    val_idx = int(len(data) * (1-val_split))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    counter = 0\n",
        "    n_labels = len(net.chars)\n",
        "    for epoch in range(n_epochs):\n",
        "        # initialize weights\n",
        "        h = net.init_hidden(batch_size)\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            # one hot encoding\n",
        "            x = one_hot_encode(x, n_labels)\n",
        "            # convert into tensor\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            # switch to GPU\n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                \n",
        "            # hidden variable state \n",
        "            h = tuple([each.data for each in h])\n",
        "            \n",
        "            # initialize gradients of all tensor to zero\n",
        "            optimizer.zero_grad()\n",
        "            # train the model\n",
        "            output, h = net(inputs, h)\n",
        "            # caluclate loss\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            # backpropogation\n",
        "            loss.backward()\n",
        "            # clipping to prevent from exploding gradient\n",
        "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch: {epoch + 1}\", f\"loss: {loss.item()}\")\n",
        "            # validation data\n",
        "#             if counter % print_every == 0:\n",
        "#                 # initialize validation hidden state\n",
        "#                 val_h = net.init_hidden(batch_size)\n",
        "                \n",
        "#                 val_losses = []\n",
        "#                 # enable evaluation mode\n",
        "#                 for x,y in get_batches(val_data, batch_size, seq_length):\n",
        "#                     x = one_hot_encode(x, n_labels)\n",
        "#                     # convert into tensor\n",
        "#                     x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "#                     # validation hidden state\n",
        "#                     val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "#                     # convert into cuda\n",
        "#                     if(train_on_gpu):\n",
        "#                         inputs, targets = x.cuda(), y.cuda()\n",
        "#                     else:\n",
        "#                         inputs, targets = x, y\n",
        "                    \n",
        "#                     # output\n",
        "#                     output, h = net(inputs, h)\n",
        "#                     # calculate validation loss\n",
        "#                     val_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "#                     val_losses.append(val_loss)\n",
        "#                 net.train()\n",
        "#                 print(\"Epoch: {}\".format(epoch+1),\n",
        "#                      \"step: {}\".format(counter),\n",
        "#                      \"loss: {:.4f}\".format(loss.item())\n",
        "#                      # \"val_loss: {:.4f}\".format(np.mean(np.array(val_losses)))\n",
        "#                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1o6ijwibNdIB",
        "colab_type": "code",
        "outputId": "052982ca-d2fe-47f1-f41d-8ddbe2e1ac68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "with open(\"gdrive/My Drive/dataset/anna.txt\") as fr:\n",
        "    text = fr.read()\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch:ii for ii, ch in int2char.items()}\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GwkaKSjlNdIQ",
        "colab_type": "code",
        "outputId": "bd6ae5b4-d835-418f-a516-9bba0f985ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "seq_length = 100\n",
        "batch_size = 128\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, n_epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 loss: 2.6174428462982178\n",
            "Epoch: 2 loss: 2.156881809234619\n",
            "Epoch: 3 loss: 1.885026216506958\n",
            "Epoch: 4 loss: 1.7152546644210815\n",
            "Epoch: 5 loss: 1.6163634061813354\n",
            "Epoch: 6 loss: 1.5335824489593506\n",
            "Epoch: 7 loss: 1.479609727859497\n",
            "Epoch: 8 loss: 1.4306859970092773\n",
            "Epoch: 9 loss: 1.404299259185791\n",
            "Epoch: 10 loss: 1.363358497619629\n",
            "Epoch: 11 loss: 1.3479340076446533\n",
            "Epoch: 12 loss: 1.3192253112792969\n",
            "Epoch: 13 loss: 1.2994418144226074\n",
            "Epoch: 14 loss: 1.2886830568313599\n",
            "Epoch: 15 loss: 1.2643039226531982\n",
            "Epoch: 16 loss: 1.235701560974121\n",
            "Epoch: 17 loss: 1.2195324897766113\n",
            "Epoch: 18 loss: 1.2094656229019165\n",
            "Epoch: 19 loss: 1.192408561706543\n",
            "Epoch: 20 loss: 1.1863821744918823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5zktYOaAtyV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save model\n",
        "checkpoint = {\n",
        "    \"n_layers\": net.n_layers,\n",
        "    \"n_hidden\": net.n_hidden,\n",
        "    \"state_dict\": net.state_dict(),\n",
        "    \"tokens\": net.chars\n",
        "}\n",
        "\n",
        "with open(\"lstm_book_writer.pth\", \"wb\") as f:\n",
        "  torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "94nBea59C5uS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(net, token, h=None, top_k=None):\n",
        "  x = np.array([[net.char2int[ch] for ch in token]])\n",
        "  x = one_hot_encode(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "  if(train_on_gpu):\n",
        "    inputs = inputs.cuda()\n",
        "    \n",
        "  h = tuple([each.data for each in h])\n",
        "  # get output from model\n",
        "  out, h = net(inputs, h)\n",
        "  \n",
        "  p = F.softmax(out, dim=1).data\n",
        "  if(train_on_gpu):\n",
        "    p = p.cpu()\n",
        "  \n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(net.chars))\n",
        "  else:\n",
        "    p, top_ch = p.topk(top_k)\n",
        "    top_ch = top_ch.numpy().squeeze()\n",
        "# select the likely next character with some element of randomness\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xg1S3BexIw-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y-HJyUrFJYrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "0efe55ce-85bb-4118-83a2-0087d7916b7e"
      },
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime=\"Anna\", top_k=5))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna, still more\n",
            "shared.'\n",
            "\n",
            "Anna heard the conviction to her.\n",
            "\n",
            "\n",
            "\n",
            "Chapter 6\n",
            "\n",
            "\n",
            "Alexay Alexandrovitch always had been satisfactin's to an attitude\n",
            "in the pavaling from their conversation with him. But at the time\n",
            "then to his wife he had an exceedingly connected that had a bare and\n",
            "three honses, and they were straight a little.\n",
            "\n",
            "\"I always have some one in her, a drear.'s and telliment for my strung\n",
            "force of his face. Then he was self-mere five on the thoughts\n",
            "of the sacre of the ways that sure he marvelous, in the mushroom and\n",
            "called up, so as to be altertald to a conversation with yathing on\n",
            "his face, and well, and that to me that I shall be in letter\n",
            "with him in them.\"\n",
            "\n",
            "\"And I'm glad you the solition of all some tried, in an\n",
            "instant at the pavilion of her own interest.\"\n",
            "\n",
            "\"Well, weat, you must go and go away, and see it all, and how, and\n",
            "is that he did not care to see me for anything but shame, because it\n",
            "seemed to me, and how do you know, there are so and settle the principal\n",
            "sincance,\" she sa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8m06xUzuJpRy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}